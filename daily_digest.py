import os
import datetime
import requests
import arxiv
import feedparser
import time
from openai import OpenAI

# ================= é…ç½®åŒºåŸŸ =================

# 1. å…³é”®è¯ç­–ç•¥ (ä¿æŒä¸å˜)
KEYWORDS_TECH = [
    "Large Language Models", "Generative AI", "AI Agents", 
    "RAG", "Transformer", "Vector Database"
]

KEYWORDS_BIZ = [
    "E-commerce", "Fintech", "Online Retail", 
    "Fraud Detection", "Supply Chain", "Personalized Recommendation",
    "Digital Banking", "Payment Gateway"
]

ALL_KEYWORDS = KEYWORDS_TECH + KEYWORDS_BIZ

# 2. RSS æº (ä¿æŒä¸å˜)
RSS_FEEDS = [
    "https://techcrunch.com/category/artificial-intelligence/feed/",
    "https://techcrunch.com/category/fintech/feed/",
    "https://techcrunch.com/category/ecommerce/feed/",
    "https://www.infoq.cn/feed",
]

# 3. æ‚¨çš„è§’è‰²ä¸Šä¸‹æ–‡
COMPANY_CONTEXT = """
èº«ä»½ï¼šä¸€å®¶äº’è”ç½‘ç”µå•†ä¸é‡‘èç§‘æŠ€å…¬å¸çš„ CTOã€‚
æ ¸å¿ƒå…³æ³¨ç‚¹ï¼š
1. **AI è½åœ°**: å¦‚ä½•ç”¨ LLM/Agent æå‡å®¢æœæ•ˆç‡ã€ä¼˜åŒ–æœç´¢æ¨èã€‚
2. **é‡‘èé£æ§**: æ–°çš„åæ¬ºè¯ˆæŠ€æœ¯ã€åˆè§„ç§‘æŠ€ã€‚
3. **ç«å“åŠ¨æ€**: äºšé©¬é€Šã€Shopifyã€Stripeã€æ”¯ä»˜å®çš„æŠ€æœ¯åŠ¨ä½œã€‚
"""

# æ—¶é—´è®¾ç½®
YESTERDAY = datetime.datetime.now() - datetime.timedelta(days=1)
UNIX_TIMESTAMP_YESTERDAY = int(time.mktime(YESTERDAY.timetuple()))

# ================= æ ¸å¿ƒä¿®æ”¹ç‚¹ï¼šé€‚é…è±†åŒ… (ç«å±±å¼•æ“) =================

# 1. è·å– Endpoint ID (è¿™æ˜¯è±†åŒ…ç‰¹æœ‰çš„)
DOUBAO_MODEL = os.environ.get("DOUBAO_ENDPOINT_ID") 

# 2. åˆå§‹åŒ–å®¢æˆ·ç«¯ (æŒ‡å‘ç«å±±å¼•æ“çš„ Base URL)
client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
    base_url="https://ark.cn-beijing.volces.com/api/v3" # ç«å±±å¼•æ“å®˜æ–¹å…¼å®¹æ¥å£
)

WEBHOOK_URL = os.environ.get("WEBHOOK_URL")

# ================= æŠ“å–å‡½æ•° (ä¿æŒä¸å˜) =================

def fetch_hacker_news():
    print("æ­£åœ¨æŠ“å– Hacker News...")
    articles = []
    query_str = " OR ".join([f'"{k}"' for k in ALL_KEYWORDS[:5]])
    url = f"http://hn.algolia.com/api/v1/search_by_date?query={query_str}&tags=story&numericFilters=created_at_i>{UNIX_TIMESTAMP_YESTERDAY}"
    try:
        res = requests.get(url).json()
        for hit in res.get('hits', [])[:5]:
            articles.append({
                "source": "Hacker News",
                "title": hit.get('title'),
                "url": hit.get('url', f"https://news.ycombinator.com/item?id={hit.get('objectID')}"),
                "summary": "N/A"
            })
    except Exception as e:
        print(f"HN æŠ“å–å¼‚å¸¸: {e}")
    return articles

def fetch_arxiv_papers():
    print("æ­£åœ¨æŠ“å– ArXiv...")
    papers = []
    search_query = " OR ".join([f'(ti:"{k}" OR abs:"{k}")' for k in KEYWORDS_TECH])
    try:
        search = arxiv.Search(
            query = f'cat:cs.AI AND ({search_query})',
            max_results = 5,
            sort_by = arxiv.SortCriterion.SubmittedDate
        )
        for result in search.results():
            if result.published.date() >= YESTERDAY.date():
                papers.append({
                    "source": "ArXiv",
                    "title": result.title,
                    "url": result.entry_id,
                    "summary": result.summary[:200].replace("\n", " ") + "..."
                })
    except Exception as e:
        print(f"ArXiv æŠ“å–å¼‚å¸¸: {e}")
    return papers

def fetch_rss_feeds():
    print("æ­£åœ¨æŠ“å– RSS...")
    articles = []
    for feed_url in RSS_FEEDS:
        try:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:3]:
                content_text = (entry.title + entry.get('summary', '')).lower()
                if any(k.lower() in content_text for k in ALL_KEYWORDS):
                    articles.append({
                        "source": f"RSS ({feed.feed.get('title', 'Media')})",
                        "title": entry.title,
                        "url": entry.link,
                        "summary": entry.get('summary', 'No summary')[:150] + "..."
                    })
        except Exception as e:
            print(f"RSS {feed_url} æŠ“å–å¼‚å¸¸: {e}")
    return articles

# ================= åˆ†æä¸æ¨é€ =================

def analyze_and_summarize(content_list):
    if not content_list:
        return None

    raw_text = ""
    for idx, item in enumerate(content_list):
        raw_text += f"{idx+1}. [{item['source']}] {item['title']}\né“¾æ¥: {item['url']}\næ‘˜è¦: {item['summary']}\n\n"

    print(f"æ­£åœ¨è°ƒç”¨è±†åŒ… ({DOUBAO_MODEL}) åˆ†æ {len(content_list)} æ¡å†…å®¹...")
    
    # --- ä¿®æ”¹ç‚¹ 1: ä¼˜åŒ– Promptï¼Œé€‚é…é£ä¹¦æ ¼å¼ ---
    prompt = f"""
    ä½ æ˜¯æˆ‘å…¬å¸çš„ã€é¦–å¸­æŠ€æœ¯æƒ…æŠ¥å®˜ã€‘ã€‚
    
    ã€æˆ‘çš„èƒŒæ™¯ã€‘
    {COMPANY_CONTEXT}
    
    ã€ä»Šæ—¥åŸå§‹æƒ…æŠ¥ã€‘
    {raw_text}
    
    ã€ä»»åŠ¡ã€‘
    è¯·ä»¥ CTO çš„æˆ˜ç•¥è§†è§’å®¡è§†ä¿¡æ¯ï¼Œå‰”é™¤å™ªéŸ³ã€‚
    
    ã€âš ï¸ æ ¼å¼ä¸¥æ ¼è¦æ±‚ (é’ˆå¯¹é£ä¹¦æ¸²æŸ“ä¼˜åŒ–)ã€‘
    1. **ç»å¯¹ä¸è¦ä½¿ç”¨** Markdown æ ‡é¢˜è¯­æ³•ï¼ˆå¦‚ #, ##, ###ï¼‰ï¼Œå› ä¸ºå®¢æˆ·ç«¯æ— æ³•æ¸²æŸ“ã€‚
    2. æ‰€æœ‰çš„æ ‡é¢˜ã€é‡ç‚¹ï¼Œè¯·ä¸€å¾‹ä½¿ç”¨ **åŒæ˜Ÿå·åŠ ç²—** (ä¾‹å¦‚ï¼š**æ ‡é¢˜**) ä»£æ›¿ã€‚
    3. åˆ—è¡¨é¡¹è¯·ä½¿ç”¨ emoji (ğŸ”¹) æˆ–åœ†ç‚¹ (â€¢) å¼€å¤´ã€‚
    
    ã€ç›®æ ‡è¾“å‡ºæ ·å¼æ¨¡æ¿ã€‘
    **ğŸš€ è¡Œä¸šä¸ä¸šåŠ¡åŠ¨æ€**
    
    **[æ ‡é¢˜æ–‡æœ¬](é“¾æ¥URL)**
    â€¢ **æƒ…æŠ¥**: è¿™é‡Œå†™æ‘˜è¦...
    â€¢ **CTO æ´å¯Ÿ**: è¿™é‡Œå†™åˆ†æ...
    
    (ç©ºä¸€è¡Œ)
    
    **âš¡ æŠ€æœ¯å‰æ²¿**
    
    **[æ ‡é¢˜æ–‡æœ¬](é“¾æ¥URL)**
    â€¢ **æƒ…æŠ¥**: è¿™é‡Œå†™æ‘˜è¦...
    â€¢ **CTO æ´å¯Ÿ**: è¿™é‡Œå†™åˆ†æ...
    
    å¦‚æœå…¨æ˜¯å™ªéŸ³ï¼Œå›å¤â€œä»Šæ—¥æ— é«˜ä»·å€¼æ›´æ–°â€ã€‚
    """

    try:
        response = client.chat.completions.create(
            model=DOUBAO_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        content = response.choices[0].message.content
        
        # --- ä¿®æ”¹ç‚¹ 2: å¼ºåˆ¶ä»£ç æ¸…æ´— (é˜²æ­¢ AI ä¸å¬è¯) ---
        # å¦‚æœ AI è¿˜æ˜¯è¾“å‡ºäº† ###ï¼Œæˆ‘ä»¬å¼ºåˆ¶æŠŠå®ƒåˆ æ‰ï¼Œæˆ–è€…æ›¿æ¢ä¸ºåŠ ç²—
        content = content.replace("### ", "").replace("## ", "").replace("###", "")
        
        return content
    except Exception as e:
        print(f"LLM è°ƒç”¨å¤±è´¥: {e}")
        return None

def send_notification(content):
    if not content: return
    
    title = f"ğŸ“… CTO æ—©æŠ¥ | {datetime.date.today()}"
    msg = {
        "msg_type": "interactive",
        "card": {
            "header": {"title": {"content": title, "tag": "plain_text"}},
            "elements": [{"tag": "markdown", "content": content}]
        }
    }
    if "hooks.slack.com" in WEBHOOK_URL:
        msg = {"text": f"*{title}*\n\n{content}"}

    try:
        requests.post(WEBHOOK_URL, json=msg)
        print("âœ… æ¨é€æˆåŠŸ")
    except Exception as e:
        print(f"æ¨é€å¤±è´¥: {e}")

if __name__ == "__main__":
    hn = fetch_hacker_news()
    arxiv = fetch_arxiv_papers()
    rss = fetch_rss_feeds()
    all_data = hn + arxiv + rss
    
    if all_data:
        report = analyze_and_summarize(all_data)
        if report and "ä»Šæ—¥æ— é«˜ä»·å€¼æ›´æ–°" not in report:
            send_notification(report)
    else:
        print("æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®ã€‚")

import os
import datetime
import requests
import arxiv
import feedparser
import time
from openai import OpenAI

# ================= 0. ç¯å¢ƒä¾èµ–æ£€æŸ¥ =================
# å»ºè®®ä½¿ç”¨ python-dotenv åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# ================= 1. é…ç½®åŒºåŸŸ =================

# å…³é”®è¯ç­–ç•¥
KEYWORDS_TECH = [
    "Large Language Models", "Generative AI", "AI Agents", 
    "RAG", "Transformer", "Vector Database"
]

KEYWORDS_BIZ = [
    "E-commerce", "Fintech", "Online Retail", 
    "Fraud Detection", "Supply Chain", "Personalized Recommendation",
    "Digital Banking", "Payment Gateway"
]

ALL_KEYWORDS = KEYWORDS_TECH + KEYWORDS_BIZ

# RSS æº
RSS_FEEDS = [
    "https://techcrunch.com/category/artificial-intelligence/feed/",
    "https://techcrunch.com/category/fintech/feed/",
    "https://techcrunch.com/category/ecommerce/feed/",
    "https://www.infoq.cn/feed",
]

# æ‚¨çš„è§’è‰²ä¸Šä¸‹æ–‡
COMPANY_CONTEXT = """
èº«ä»½ï¼šä¸€å®¶äº’è”ç½‘ç”µå•†ä¸é‡‘èç§‘æŠ€å…¬å¸çš„ CTOã€‚
æ ¸å¿ƒå…³æ³¨ç‚¹ï¼š
1. **AI è½åœ°**: å¦‚ä½•ç”¨ LLM/Agent æå‡å®¢æœæ•ˆç‡ã€ä¼˜åŒ–æœç´¢æ¨èã€‚
2. **é‡‘èé£æ§**: æ–°çš„åæ¬ºè¯ˆæŠ€æœ¯ã€åˆè§„ç§‘æŠ€ã€‚
3. **ç«å“åŠ¨æ€**: äºšé©¬é€Šã€Shopifyã€Stripeã€æ”¯ä»˜å®çš„æŠ€æœ¯åŠ¨ä½œã€‚
"""

# æ—¶é—´è®¾ç½®
YESTERDAY = datetime.datetime.now() - datetime.timedelta(days=1)
UNIX_TIMESTAMP_YESTERDAY = int(time.mktime(YESTERDAY.timetuple()))

# API é…ç½® (è¯·ç¡®ä¿ç¯å¢ƒå˜é‡å·²è®¾ç½®)
DOUBAO_MODEL = os.environ.get("DOUBAO_ENDPOINT_ID") # ä¾‹å¦‚ ep-2024...
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
WEBHOOK_URL = os.environ.get("WEBHOOK_URL")

# åˆå§‹åŒ–å®¢æˆ·ç«¯ (ç«å±±å¼•æ“)
client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url="https://ark.cn-beijing.volces.com/api/v3"
)

# ================= 2. æŠ“å–å‡½æ•° =================

def fetch_hacker_news():
    print("ğŸ” æ­£åœ¨æŠ“å– Hacker News...")
    articles = []
    query_str = " OR ".join([f'"{k}"' for k in ALL_KEYWORDS[:5]])
    url = f"http://hn.algolia.com/api/v1/search_by_date?query={query_str}&tags=story&numericFilters=created_at_i>{UNIX_TIMESTAMP_YESTERDAY}"
    try:
        res = requests.get(url, timeout=10).json()
        for hit in res.get('hits', [])[:5]:
            articles.append({
                "source": "Hacker News",
                "title": hit.get('title'),
                "url": hit.get('url', f"https://news.ycombinator.com/item?id={hit.get('objectID')}"),
                "summary": "N/A"
            })
    except Exception as e:
        print(f"âŒ HN æŠ“å–å¼‚å¸¸: {e}")
    return articles

def fetch_arxiv_papers():
    print("ğŸ” æ­£åœ¨æŠ“å– ArXiv...")
    papers = []
    # æ„é€ æŸ¥è¯¢ï¼šcs.AI ç±»åˆ« AND (å…³é”®è¯)
    search_query = " OR ".join([f'(ti:"{k}" OR abs:"{k}")' for k in KEYWORDS_TECH])
    try:
        # æ³¨æ„ï¼šarxiv åº“å¯èƒ½æœ‰ API é™åˆ¶ï¼Œå»ºè®®ç”Ÿäº§ç¯å¢ƒå¢åŠ é‡è¯•æœºåˆ¶
        search = arxiv.Search(
            query = f'cat:cs.AI AND ({search_query})',
            max_results = 5,
            sort_by = arxiv.SortCriterion.SubmittedDate
        )
        for result in search.results():
            if result.published.date() >= YESTERDAY.date():
                papers.append({
                    "source": "ArXiv",
                    "title": result.title,
                    "url": result.entry_id,
                    "summary": result.summary[:200].replace("\n", " ") + "..."
                })
    except Exception as e:
        print(f"âŒ ArXiv æŠ“å–å¼‚å¸¸: {e}")
    return papers

def fetch_rss_feeds():
    print("ğŸ” æ­£åœ¨æŠ“å– RSS...")
    articles = []
    for feed_url in RSS_FEEDS:
        try:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:3]:
                content_text = (entry.title + entry.get('summary', '')).lower()
                if any(k.lower() in content_text for k in ALL_KEYWORDS):
                    articles.append({
                        "source": f"RSS ({feed.feed.get('title', 'Media')})",
                        "title": entry.title,
                        "url": entry.link,
                        "summary": entry.get('summary', 'No summary')[:150] + "..."
                    })
        except Exception as e:
            print(f"âŒ RSS {feed_url} æŠ“å–å¼‚å¸¸: {e}")
    return articles

# ================= 3. åˆ†æä¸æ¨é€ =================

def analyze_and_summarize(content_list):
    if not content_list:
        return None

    raw_text = ""
    for idx, item in enumerate(content_list):
        raw_text += f"{idx+1}. [{item['source']}] {item['title']}\né“¾æ¥: {item['url']}\næ‘˜è¦: {item['summary']}\n\n"

    print(f"ğŸ¤– æ­£åœ¨è°ƒç”¨è±†åŒ… ({DOUBAO_MODEL}) åˆ†æ {len(content_list)} æ¡å†…å®¹...")
    
    prompt = f"""
    ä½ æ˜¯æˆ‘å…¬å¸çš„ã€é¦–å¸­æŠ€æœ¯æƒ…æŠ¥å®˜ã€‘ã€‚
    
    ã€æˆ‘çš„èƒŒæ™¯ã€‘
    {COMPANY_CONTEXT}
    
    ã€ä»Šæ—¥åŸå§‹æƒ…æŠ¥ã€‘
    {raw_text}
    
    ã€ä»»åŠ¡ã€‘
    è¯·ä»¥ CTO çš„æˆ˜ç•¥è§†è§’å®¡è§†ä¿¡æ¯ï¼Œå‰”é™¤å™ªéŸ³ã€‚
    
    ã€âš ï¸ æ ¼å¼ä¸¥æ ¼è¦æ±‚ (é’ˆå¯¹é£ä¹¦æ¸²æŸ“ä¼˜åŒ–)ã€‘
    1. **ç»å¯¹ä¸è¦ä½¿ç”¨** Markdown æ ‡é¢˜è¯­æ³•ï¼ˆå¦‚ #, ##, ###ï¼‰ï¼Œå› ä¸ºå®¢æˆ·ç«¯æ— æ³•æ¸²æŸ“ã€‚
    2. æ‰€æœ‰çš„æ ‡é¢˜ã€é‡ç‚¹ï¼Œè¯·ä¸€å¾‹ä½¿ç”¨ **åŒæ˜Ÿå·åŠ ç²—** (ä¾‹å¦‚ï¼š**æ ‡é¢˜**) ä»£æ›¿ã€‚
    3. åˆ—è¡¨é¡¹è¯·ä½¿ç”¨ emoji (ğŸ”¹) æˆ–åœ†ç‚¹ (â€¢) å¼€å¤´ã€‚
    4. æ¯ä¸ªæ¿å—ä¹‹é—´è¯·ç•™å‡ºç©ºè¡Œã€‚
    
    ã€ç›®æ ‡è¾“å‡ºæ ·å¼æ¨¡æ¿ã€‘
    **ğŸš€ è¡Œä¸šä¸ä¸šåŠ¡åŠ¨æ€**
    
    **[æ ‡é¢˜æ–‡æœ¬](é“¾æ¥URL)**
    â€¢ **æƒ…æŠ¥**: è¿™é‡Œå†™æ‘˜è¦...
    â€¢ **CTO æ´å¯Ÿ**: è¿™é‡Œå†™åˆ†æ...
    
    (ç©ºä¸€è¡Œ)
    
    **âš¡ æŠ€æœ¯å‰æ²¿**
    
    **[æ ‡é¢˜æ–‡æœ¬](é“¾æ¥URL)**
    â€¢ **æƒ…æŠ¥**: è¿™é‡Œå†™æ‘˜è¦...
    â€¢ **CTO æ´å¯Ÿ**: è¿™é‡Œå†™åˆ†æ...
    
    å¦‚æœå…¨æ˜¯å™ªéŸ³ï¼Œå›å¤â€œä»Šæ—¥æ— é«˜ä»·å€¼æ›´æ–°â€ã€‚
    """

    try:
        response = client.chat.completions.create(
            model=DOUBAO_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        content = response.choices[0].message.content
        
        # --- æ•°æ®æ¸…æ´— ---
        # 1. ç§»é™¤ Markdown æ ‡é¢˜ç¬¦ï¼Œé˜²æ­¢æ ¼å¼é”™ä¹±
        content = content.replace("### ", "").replace("## ", "").replace("###", "")
        # 2. ä¼˜åŒ–åˆ—è¡¨é—´è·ï¼Œç¡®ä¿é£ä¹¦æ¸²æŸ“ä¸æ‹¥æŒ¤
        content = content.replace("\nâ€¢", "\n\nâ€¢").replace("\nğŸ”¹", "\n\nğŸ”¹")
        
        return content
    except Exception as e:
        print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
        return None

def send_notification(content):
    if not content: return
    
    today_str = datetime.date.today().strftime("%Y-%m-%d")
    title = f"ğŸ“… CTO æ—©æŠ¥ | {today_str}"
    
    # æ„é€ é£ä¹¦äº¤äº’å¼å¡ç‰‡
    msg = {
        "msg_type": "interactive",
        "card": {
            "header": {
                "title": {
                    "content": title,
                    "tag": "plain_text"
                }
            },
            "elements": [
                {
                    "tag": "markdown",
                    "content": content
                },
                {
                    "tag": "note",
                    "elements": [
                        {
                            "tag": "plain_text",
                            "content": "Powered by Doubao & Feishu Bot"
                        }
                    ]
                }
            ]
        }
    }

    # å…¼å®¹ Slack (å¦‚æœ URL åŒ…å« slack)
    if WEBHOOK_URL and "hooks.slack.com" in WEBHOOK_URL:
        msg = {"text": f"*{title}*\n\n{content}"}

    try:
        # âœ… å…³é”®ä¿®æ­£ï¼šç›´æ¥ä½¿ç”¨ json=msgï¼Œä¸è¦ json.dumps
        resp = requests.post(WEBHOOK_URL, json=msg)
        resp.raise_for_status() # æ£€æŸ¥ HTTP é”™è¯¯
        print(f"âœ… æ¨é€æˆåŠŸ! å“åº”: {resp.json()}")
    except Exception as e:
        print(f"âŒ æ¨é€å¤±è´¥: {e}")

# ================= 4. ä¸»ç¨‹åºå…¥å£ =================

if __name__ == "__main__":
    if not WEBHOOK_URL or not DOUBAO_MODEL:
        print("âš ï¸ è­¦å‘Š: ç¯å¢ƒå˜é‡ WEBHOOK_URL æˆ– DOUBAO_ENDPOINT_ID æœªè®¾ç½®ï¼Œç¨‹åºå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚")

    print("ğŸš€ ä»»åŠ¡å¼€å§‹...")
    
    # 1. è·å–æ•°æ®
    hn_data = fetch_hacker_news()
    # âš ï¸ ä¿®æ­£ï¼šå˜é‡åæ”¹ä¸º arxiv_dataï¼Œé¿å…è¦†ç›–å¯¼å…¥çš„ arxiv æ¨¡å—
    arxiv_data = fetch_arxiv_papers() 
    rss_data = fetch_rss_feeds()
    
    all_data = hn_data + arxiv_data + rss_data
    
    if all_data:
        print(f"ğŸ“Š å…±è·å– {len(all_data)} æ¡åŸå§‹æ•°æ®ï¼Œå¼€å§‹åˆ†æ...")
        report = analyze_and_summarize(all_data)
        
        if report and "ä»Šæ—¥æ— é«˜ä»·å€¼æ›´æ–°" not in report:
            send_notification(report)
        else:
            print("ğŸ”• ä»Šæ—¥æ— é«˜ä»·å€¼å†…å®¹ï¼Œè·³è¿‡æ¨é€ã€‚")
    else:
        print("ğŸ“­ æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®ã€‚")

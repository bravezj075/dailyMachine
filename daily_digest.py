import os
import datetime
import requests
import arxiv
import feedparser
import time
from openai import OpenAI

# ================= é…ç½®åŒºåŸŸ =================

# 1. å…³é”®è¯ç­–ç•¥
# æŠ€æœ¯ä¾§ï¼šå…³æ³¨ AI æ ¸å¿ƒèƒ½åŠ›
KEYWORDS_TECH = [
    "Large Language Models", "Generative AI", "AI Agents", 
    "RAG", "Transformer", "Vector Database"
]

# ä¸šåŠ¡ä¾§ï¼šå…³æ³¨ ç”µå•† & é‡‘è åœºæ™¯
KEYWORDS_BIZ = [
    "E-commerce", "Fintech", "Online Retail", 
    "Fraud Detection", "Supply Chain", "Personalized Recommendation",
    "Digital Banking", "Payment Gateway"
]

# åˆå¹¶ç”¨äºæ··åˆæœç´¢
ALL_KEYWORDS = KEYWORDS_TECH + KEYWORDS_BIZ

# 2. å•†ä¸šåª’ä½“ RSS æº (æ•æ‰è¡Œä¸šåŠ¨æ€)
RSS_FEEDS = [
    "https://techcrunch.com/category/artificial-intelligence/feed/",
    "https://techcrunch.com/category/fintech/feed/",
    "https://techcrunch.com/category/ecommerce/feed/",
    "https://www.infoq.cn/feed", # InfoQ ä¸­æ–‡ç«™ (å¯é€‰ï¼Œè¦†ç›–æ¶æ„ä¸æŠ€æœ¯ç®¡ç†)
]

# 3. æ‚¨çš„è§’è‰²ä¸Šä¸‹æ–‡ (AI ç­›é€‰çš„æ ¸å¿ƒä¾æ®)
COMPANY_CONTEXT = """
èº«ä»½ï¼šä¸€å®¶äº’è”ç½‘ç”µå•†ä¸é‡‘èç§‘æŠ€å…¬å¸çš„ CTOã€‚
æ ¸å¿ƒå…³æ³¨ç‚¹ï¼š
1. **AI è½åœ°**: å¦‚ä½•ç”¨ LLM/Agent æå‡å®¢æœæ•ˆç‡ã€ä¼˜åŒ–æœç´¢æ¨èã€ç”Ÿæˆè¥é”€å†…å®¹ã€‚
2. **é‡‘èé£æ§**: æ–°çš„åæ¬ºè¯ˆæŠ€æœ¯ã€åˆè§„ç§‘æŠ€ã€æ”¯ä»˜å®‰å…¨ã€‚
3. **ç«å“åŠ¨æ€**: äºšé©¬é€Šã€Shopifyã€Stripeã€æ”¯ä»˜å®ç­‰å·¨å¤´çš„æœ€æ–°æŠ€æœ¯åŠ¨ä½œã€‚
4. **æ¶æ„æ¼”è¿›**: é™æœ¬å¢æ•ˆï¼Œä»å•ä½“å‘å¾®æœåŠ¡/Serverless çš„è¿ç§»ä¸æ²»ç†ã€‚
"""

# æ—¶é—´è®¾ç½®
YESTERDAY = datetime.datetime.now() - datetime.timedelta(days=1)
UNIX_TIMESTAMP_YESTERDAY = int(time.mktime(YESTERDAY.timetuple()))

# åˆå§‹åŒ–å®¢æˆ·ç«¯
# client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
# ä¿®æ”¹ä¸º ğŸ‘‡ (æ³¨æ„ base_url)
client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"), # è¿™é‡Œçš„ Key æ¢æˆ DeepSeek çš„
    base_url="https://api.deepseek.com"       # æŒ‡å‘ DeepSeek çš„æœåŠ¡å™¨
)
WEBHOOK_URL = os.environ.get("WEBHOOK_URL")

# ================= æŠ“å–å‡½æ•° =================

def fetch_hacker_news():
    """Hacker News: ä¾§é‡æŠ€æœ¯ç¤¾åŒºçš„æ·±åº¦è®¨è®º"""
    print("æ­£åœ¨æŠ“å– Hacker News...")
    articles = []
    # ä¸ºäº†é¿å… URL è¿‡é•¿ï¼Œåªå–æœ€é‡è¦çš„å‰ 5 ä¸ªå…³é”®è¯è¿›è¡Œ HN æœç´¢
    query_str = " OR ".join([f'"{k}"' for k in ALL_KEYWORDS[:5]])
    
    url = f"http://hn.algolia.com/api/v1/search_by_date?query={query_str}&tags=story&numericFilters=created_at_i>{UNIX_TIMESTAMP_YESTERDAY}"
    
    try:
        res = requests.get(url).json()
        for hit in res.get('hits', [])[:5]:
            articles.append({
                "source": "Hacker News",
                "title": hit.get('title'),
                "url": hit.get('url', f"https://news.ycombinator.com/item?id={hit.get('objectID')}"),
                "summary": "N/A (Community Discussion)"
            })
    except Exception as e:
        print(f"HN æŠ“å–å¼‚å¸¸: {e}")
    return articles

def fetch_arxiv_papers():
    """ArXiv: ä¾§é‡ AI æŠ€æœ¯çš„æœ€å‰æ²¿ç†è®º"""
    print("æ­£åœ¨æŠ“å– ArXiv...")
    papers = []
    # ArXiv åªæœç´¢æŠ€æœ¯å…³é”®è¯
    search_query = " OR ".join([f'(ti:"{k}" OR abs:"{k}")' for k in KEYWORDS_TECH])
    
    try:
        search = arxiv.Search(
            query = f'cat:cs.AI AND ({search_query})',
            max_results = 5,
            sort_by = arxiv.SortCriterion.SubmittedDate
        )
        for result in search.results():
            if result.published.date() >= YESTERDAY.date():
                papers.append({
                    "source": "ArXiv",
                    "title": result.title,
                    "url": result.entry_id,
                    "summary": result.summary[:200].replace("\n", " ") + "..."
                })
    except Exception as e:
        print(f"ArXiv æŠ“å–å¼‚å¸¸: {e}")
    return papers

def fetch_rss_feeds():
    """RSS: ä¾§é‡å•†ä¸šè½åœ°å’Œè¡Œä¸šæ–°é—»"""
    print("æ­£åœ¨æŠ“å– RSS...")
    articles = []
    for feed_url in RSS_FEEDS:
        try:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:3]: # æ¯ä¸ªæºå–å‰3æ¡
                content_text = (entry.title + entry.get('summary', '')).lower()
                # ç®€å•è¿‡æ»¤ï¼šåªè¦åŒ…å«æˆ‘ä»¬å…³å¿ƒçš„ä»»ä¸€å…³é”®è¯
                if any(k.lower() in content_text for k in ALL_KEYWORDS):
                    articles.append({
                        "source": f"RSS ({feed.feed.get('title', 'Media')})",
                        "title": entry.title,
                        "url": entry.link,
                        "summary": entry.get('summary', 'No summary')[:150] + "..."
                    })
        except Exception as e:
            print(f"RSS {feed_url} æŠ“å–å¼‚å¸¸: {e}")
    return articles

# ================= åˆ†æä¸æ¨é€ =================

def analyze_and_summarize(content_list):
    if not content_list:
        return None

    raw_text = ""
    for idx, item in enumerate(content_list):
        raw_text += f"{idx+1}. [{item['source']}] {item['title']}\né“¾æ¥: {item['url']}\næ‘˜è¦: {item['summary']}\n\n"

    print(f"æ­£åœ¨è°ƒç”¨ LLM åˆ†æ {len(content_list)} æ¡å†…å®¹...")
    
    prompt = f"""
    ä½ æ˜¯æˆ‘å…¬å¸çš„ã€é¦–å¸­æŠ€æœ¯æƒ…æŠ¥å®˜ã€‘ã€‚
    
    ã€æˆ‘çš„èƒŒæ™¯ã€‘
    {COMPANY_CONTEXT}
    
    ã€ä»Šæ—¥åŸå§‹æƒ…æŠ¥ã€‘
    {raw_text}
    
    ã€ä»»åŠ¡ã€‘
    è¯·ä»¥ CTO çš„æˆ˜ç•¥è§†è§’å®¡è§†ä¸Šè¿°ä¿¡æ¯ï¼Œå‰”é™¤å™ªéŸ³ï¼Œåªä¿ç•™å¯¹ä¸šåŠ¡æˆ–æŠ€æœ¯æ¶æ„æœ‰**å®è´¨å½±å“**çš„å†…å®¹ã€‚
    
    ã€è¾“å‡ºæ ¼å¼ (Markdown)ã€‘
    è¯·æŒ‰ç…§ä»¥ä¸‹åˆ†ç±»è¾“å‡ºï¼ˆå¦‚æœæ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯¥åˆ†ç±»å¯ç•™ç©ºï¼‰ï¼š
    
    ### ğŸš€ è¡Œä¸šä¸ä¸šåŠ¡åŠ¨æ€ (ç”µå•†/é‡‘è)
    * **[æ ‡é¢˜](é“¾æ¥)**
      * **æƒ…æŠ¥**: ä¸€å¥è¯æ¦‚æ‹¬å‘ç”Ÿäº†ä»€ä¹ˆï¼ˆå¦‚ï¼šStripe æ¨å‡ºäº†æ–°åŠŸèƒ½...ï¼‰ã€‚
      * **CTO æ´å¯Ÿ**: å¯¹æˆ‘ä»¬ä¸šåŠ¡çš„å€Ÿé‰´æ„ä¹‰ï¼ˆå¦‚ï¼šæˆ‘ä»¬å¯ä»¥æ¨¡ä»¿è¿™ä¸ªåšé£æ§...ï¼‰ã€‚
    
    ### âš¡ æŠ€æœ¯å‰æ²¿ (AI/æ¶æ„)
    * **[æ ‡é¢˜](é“¾æ¥)**
      * **æƒ…æŠ¥**: è§£å†³äº†ä»€ä¹ˆæŠ€æœ¯éš¾é¢˜ã€‚
      * **CTO æ´å¯Ÿ**: å®æ–½éš¾åº¦ä¸æ½œåœ¨æ”¶ç›Šï¼ˆå¦‚ï¼šé€‚åˆä½œä¸º Q3 çš„æŠ€æœ¯é¢„ç ”é¡¹ç›®...ï¼‰ã€‚

    > **æ€»ç»“**: (å¯é€‰) å¦‚æœæœ‰ç‰¹åˆ«é‡å¤§çš„æ¶ˆæ¯ï¼Œç”¨åŠ ç²—ä¸€å¥è¯æé†’æˆ‘ã€‚
    
    å¦‚æœä»Šå¤©å…¨æ˜¯æ— å…³å™ªéŸ³ï¼Œè¯·ç›´æ¥å›å¤ï¼šâ€œä»Šæ—¥æ— é«˜ä»·å€¼æ›´æ–°ã€‚â€
    """

    try:
        response = client.chat.completions.create(
           # model="gpt-4o",
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"LLM è°ƒç”¨å¤±è´¥: {e}")
        return None

def send_notification(content):
    if not content:
        return
    
    # æ ‡é¢˜å¢åŠ æ—¥æœŸ
    title = f"ğŸ“… CTO æ—©æŠ¥ | {datetime.date.today()}"
    
    # é€‚é…é£ä¹¦ Webhook
    msg = {
        "msg_type": "interactive",
        "card": {
            "header": {"title": {"content": title, "tag": "plain_text"}},
            "elements": [{"tag": "markdown", "content": content}]
        }
    }
    
    # ç®€å•çš„ Slack å…¼å®¹ (å¦‚æœ URL åŒ…å« slack)
    if "hooks.slack.com" in WEBHOOK_URL:
        msg = {"text": f"*{title}*\n\n{content}"}

    try:
        requests.post(WEBHOOK_URL, json=msg)
        print("âœ… æ¨é€æˆåŠŸ")
    except Exception as e:
        print(f"æ¨é€å¤±è´¥: {e}")

# ================= ä¸»å…¥å£ =================

if __name__ == "__main__":
    # 1. èšåˆå¤šæºæ•°æ®
    hn = fetch_hacker_news()
    arxiv = fetch_arxiv_papers()
    rss = fetch_rss_feeds()
    
    all_data = hn + arxiv + rss
    
    print(f"æŠ“å–ç»“æŸã€‚HN:{len(hn)}, ArXiv:{len(arxiv)}, RSS:{len(rss)}ã€‚æ€»è®¡: {len(all_data)}")
    
    # 2. LLM åˆ†æ
    if all_data:
        report = analyze_and_summarize(all_data)
        
        # 3. æ¨é€ç»“æœ
        if report and "ä»Šæ—¥æ— é«˜ä»·å€¼æ›´æ–°" not in report:
            send_notification(report)
        else:
            print("å†…å®¹ç»è¿‡ AI ç­›é€‰åæ— é«˜ä»·å€¼ä¿¡æ¯ï¼Œè·³è¿‡æ¨é€ã€‚")
    else:
        print("æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®ã€‚")
